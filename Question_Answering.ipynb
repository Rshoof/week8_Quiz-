{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bd3ff36d",
      "metadata": {
        "id": "bd3ff36d"
      },
      "source": [
        "# Fine-tuning Question Answering Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "605a111a",
      "metadata": {
        "id": "605a111a"
      },
      "source": [
        "This exam will guide you through loading, preprocessing, and fine-tuning a pre-trained model for a question-answering task using a dataset. Follow the steps carefully.\n",
        "\n",
        "### Model and Dataset Information\n",
        "\n",
        "For this task, you will be working with the following:\n",
        "\n",
        "- **Model Checkpoint**: Use the pre-trained model checkpoint `distilbert-base-cased` for both the model and tokenizer.\n",
        "- **Dataset**: You will be using the `christti/squad-augmented-v2` dataset. Ensure to load and preprocess the dataset correctly for training and evaluation.\n",
        "\n",
        "**Note:**\n",
        "- Any additional steps or methods you include that improve or enhance the results will be rewarded with bonus points if they are justified.\n",
        "- The steps outlined here are suggestions. You are free to implement alternative methods or approaches to achieve the task, as long as you explain the reasoning and the process at the bottom of the notebook.\n",
        "- You can use either TensorFlow or PyTorch for this task. If you prefer TensorFlow, feel free to use it when working with Hugging Face Transformers.\n",
        "- The number of data samples you choose to work with is flexible. However, if you select a very low number of samples and the training time is too short, this could affect the evaluation of your work."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a124cdfe",
      "metadata": {
        "id": "a124cdfe"
      },
      "source": [
        "## Step 1: Load the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35e3d2ee",
      "metadata": {
        "id": "35e3d2ee"
      },
      "source": [
        "Load the dataset and split it into training and test sets. Use 20% of the data for testing."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhFwL-z1NUjC",
        "outputId": "f75c20f9-09bd-4d5d-aa57-5d9987ef1f47"
      },
      "id": "IhFwL-z1NUjC",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.0.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.0.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"christti/squad-augmented-v2\", split=\"train[:1%]\")\n",
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXz08LuRNNj4",
        "outputId": "84850ea9-8ed4-4131-ea2a-f8a056d3ac25"
      },
      "id": "DXz08LuRNNj4",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "    num_rows: 1692\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ds = ds.remove_columns(['id','title'])"
      ],
      "metadata": {
        "id": "BPHEwXMROaNK"
      },
      "id": "BPHEwXMROaNK",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''ds_train = ds['train']\n",
        "ds_val = ds['validation']'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "uuxOcHJqPKh8",
        "outputId": "603a1668-9d32-45ee-bad6-4460a5672653"
      },
      "id": "uuxOcHJqPKh8",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"ds_train = ds['train']\\nds_val = ds['validation']\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''ds_train = ds_train.train_test_split(test_size=0.2)\n",
        "ds_train'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ttbagY6pR9vg",
        "outputId": "6d24e140-3750-4b9c-938a-ff47fb8d4b2e"
      },
      "id": "ttbagY6pR9vg",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ds_train = ds_train.train_test_split(test_size=0.2)\\nds_train'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a03cdcf7",
      "metadata": {
        "id": "a03cdcf7"
      },
      "source": [
        "## Step 2: Load the Pretrained Tokenizer and Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0a316837",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a316837",
        "outputId": "35511dae-b3d8-4f95-ad14-9025f411c733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        }
      ],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"question-answering\", model=\"distilbert/distilbert-base-cased-distilled-squad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "75ccbb9a",
      "metadata": {
        "id": "75ccbb9a"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert/distilbert-base-cased-distilled-squad\")\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-cased-distilled-squad\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d09dc46",
      "metadata": {
        "id": "6d09dc46"
      },
      "source": [
        "Use the model and tokenizer for the question-answering task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31a73963",
      "metadata": {
        "id": "31a73963"
      },
      "source": [
        "## Step 3: Preprocess the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d84cb15",
      "metadata": {
        "id": "9d84cb15"
      },
      "source": [
        "Define a function to preprocess the dataset by tokenizing both the context and the question. The function will also calculate the start and end positions of the answers. In the tokenizer you might face a problem if you use `truncation=True` so consider using `truncation='only_first'` if needed."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(examples):\n",
        "    inputs = tokenizer(\n",
        "        examples['context'],\n",
        "        examples['question'],\n",
        "        truncation='only_first',\n",
        "        padding='max_length',\n",
        "        max_length=400\n",
        "    )\n",
        "\n",
        "    # Tokenize the answer separately\n",
        "    answers = examples['answers']\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, answer in enumerate(answers):\n",
        "        answer_text = answer['text'][0]\n",
        "        start_char = answer['answer_start'][0]\n",
        "\n",
        "        # Tokenize the context\n",
        "        context = examples['context'][i]\n",
        "        tokenized_context = tokenizer(context, truncation=True, padding='max_length', max_length=400)\n",
        "\n",
        "        # Tokenize the answer\n",
        "        tokenized_answer = tokenizer(answer_text, truncation=True, padding='max_length', max_length=400)\n",
        "\n",
        "        # Find the token indices corresponding to the start and end of the answer\n",
        "        start_pos = None\n",
        "        end_pos = None\n",
        "\n",
        "        # Loop through the tokenized context and look for the answer tokens\n",
        "        for idx in range(len(tokenized_context['input_ids']) - len(tokenized_answer['input_ids']) + 1):\n",
        "            if tokenized_context['input_ids'][idx:idx + len(tokenized_answer['input_ids'])] == tokenized_answer['input_ids']:\n",
        "                start_pos = idx\n",
        "                end_pos = idx + len(tokenized_answer['input_ids']) - 1\n",
        "                break\n",
        "\n",
        "        if start_pos is None or end_pos is None:\n",
        "            start_pos = 0\n",
        "            end_pos = 0\n",
        "\n",
        "        start_positions.append(start_pos)\n",
        "        end_positions.append(end_pos)\n",
        "\n",
        "    inputs.update({\n",
        "        'start_positions': start_positions,\n",
        "        'end_positions': end_positions\n",
        "    })\n",
        "\n",
        "    return inputs\n",
        "\n",
        "tokenized_train_ds = ds.map(preprocess_data, batched=True, remove_columns=['id', 'title'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8fdb1bd23b4e432e92ae4f5a4430346c",
            "6fc4b37ad87a465681aaed3d4322b319",
            "afcbea8b7e4440a2a7098f7ffc1555ed",
            "3f25d70c018f488abc5bc733144689e5",
            "7912d02e6a474833af5f32dd4441598f",
            "a4ec0b465bb248f3a18ea7526ca1669f",
            "3f6568fe48f74d3b9a5c6c31bcc1ed55",
            "e67753049a9f4eca96e061f4b57a0f97",
            "2665f93c3e124817835ced3b89655252",
            "4482553195e745eeaca204c9e22c75c3",
            "3f73d079a0bd4079b010d2a3b39bfed2"
          ]
        },
        "id": "EJle3m2vYawX",
        "outputId": "ea158fae-45be-4dad-9358-b30d9419b96a"
      },
      "id": "EJle3m2vYawX",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1692 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fdb1bd23b4e432e92ae4f5a4430346c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(tokenized_train_ds, shuffle=True, batch_size=8)\n",
        "#valid_data = DataLoader(ds_val, batch_size=8)"
      ],
      "metadata": {
        "id": "PjHbatqZfw4-"
      },
      "id": "PjHbatqZfw4-",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fc7ff9c8",
      "metadata": {
        "id": "fc7ff9c8"
      },
      "source": [
        "## Step 4: Define Training Arguments and Initialize the Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a55e7d4",
      "metadata": {
        "id": "8a55e7d4"
      },
      "source": [
        "Set up the training configuration with parameters like learning rate, batch size, and number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3ad7aa6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ad7aa6b",
        "outputId": "25b69f55-0019-4f62-dfd7-ec6d3aad8d36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"no\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_ds\n",
        ")"
      ],
      "metadata": {
        "id": "ofJKVghLfkUj"
      },
      "id": "ofJKVghLfkUj",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "746d2fa8",
      "metadata": {
        "id": "746d2fa8"
      },
      "source": [
        "## Step 5: Fine-tune the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1189ec73",
      "metadata": {
        "id": "1189ec73"
      },
      "source": [
        "Run the training process using the initialized trainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "efda07d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "efda07d5",
        "outputId": "cc19b088-65cc-4ffb-e372-d16ca5637333"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='212' max='212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [212/212 01:09, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=212, training_loss=0.018126299921071756, metrics={'train_runtime': 72.1025, 'train_samples_per_second': 23.467, 'train_steps_per_second': 2.94, 'total_flos': 172707066604800.0, 'train_loss': 0.018126299921071756, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1df1eaf4",
      "metadata": {
        "id": "1df1eaf4"
      },
      "source": [
        "## Step 6: Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57095255",
      "metadata": {
        "id": "57095255"
      },
      "source": [
        "Once the model is trained, perform inference by answering a question based on a context. Use the tokenizer to process the input, and then feed it into the model to get the predicted answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "3caa026e",
      "metadata": {
        "id": "3caa026e"
      },
      "outputs": [],
      "source": [
        "question = 'How many programming languages does BLOOM support?'\n",
        "context = 'BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages.'\n",
        "\n",
        "#input = tokenizer(question_ex, context_ex, return_tensors='pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(question, context, return_tensors='pt').input_ids\n",
        "\n",
        "\n",
        "#answer = model(question=question, context=context)\n",
        "answer = pipe(question=question, context=context)\n",
        "print(\"Answer:\", answer['answer'])"
      ],
      "metadata": {
        "id": "cUA0hGoUiqgE",
        "outputId": "6cf2619b-5f04-442a-c76a-b847645289ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cUA0hGoUiqgE",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R243sAJFqu3G"
      },
      "id": "R243sAJFqu3G",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8fdb1bd23b4e432e92ae4f5a4430346c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6fc4b37ad87a465681aaed3d4322b319",
              "IPY_MODEL_afcbea8b7e4440a2a7098f7ffc1555ed",
              "IPY_MODEL_3f25d70c018f488abc5bc733144689e5"
            ],
            "layout": "IPY_MODEL_7912d02e6a474833af5f32dd4441598f"
          }
        },
        "6fc4b37ad87a465681aaed3d4322b319": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4ec0b465bb248f3a18ea7526ca1669f",
            "placeholder": "​",
            "style": "IPY_MODEL_3f6568fe48f74d3b9a5c6c31bcc1ed55",
            "value": "Map: 100%"
          }
        },
        "afcbea8b7e4440a2a7098f7ffc1555ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e67753049a9f4eca96e061f4b57a0f97",
            "max": 1692,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2665f93c3e124817835ced3b89655252",
            "value": 1692
          }
        },
        "3f25d70c018f488abc5bc733144689e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4482553195e745eeaca204c9e22c75c3",
            "placeholder": "​",
            "style": "IPY_MODEL_3f73d079a0bd4079b010d2a3b39bfed2",
            "value": " 1692/1692 [00:22&lt;00:00, 74.23 examples/s]"
          }
        },
        "7912d02e6a474833af5f32dd4441598f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4ec0b465bb248f3a18ea7526ca1669f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f6568fe48f74d3b9a5c6c31bcc1ed55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e67753049a9f4eca96e061f4b57a0f97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2665f93c3e124817835ced3b89655252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4482553195e745eeaca204c9e22c75c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f73d079a0bd4079b010d2a3b39bfed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}